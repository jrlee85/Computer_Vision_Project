{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c763c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from random import sample\n",
    "import shutil\n",
    "import glob\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef325c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Increase the display size in order to prevent truncation later on\n",
    "\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc646f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to read in json metadata for the chosen datasets as data frame\n",
    "\n",
    "def metadata_to_df(file, dataset):\n",
    "    with open(file) as json_data:\n",
    "        json_dict = json.load(json_data)\n",
    "    df_images = pd.DataFrame(json_dict['images'])\n",
    "    df_images = df_images.set_index('id')\n",
    "    df_annot = pd.DataFrame(json_dict['annotations'])\n",
    "    df_annot = df_annot.set_index('image_id')\n",
    "    df_annot = df_annot.drop(['id'], axis=1)\n",
    "    df_annot.index.name = 'id'\n",
    "    df = pd.merge(df_annot, df_images,  how='left', on='id')\n",
    "    df = df[['category_id', 'file_name']]\n",
    "    df_cats = pd.DataFrame(json_dict['categories'])\n",
    "    for col in ['count', 'species', 'genus', 'family',\n",
    "            'ord', 'class', 'common name']:\n",
    "        if col in df_cats.columns:\n",
    "            df_cats = df_cats.drop(col, axis=1)\n",
    "    df_cats = df_cats.rename(columns={'id':'category_id'})\n",
    "    df = df.reset_index().merge(df_cats, how=\"left\").set_index('id')\n",
    "    df['name'].value_counts()\n",
    "    df[\"Dataset\"] = dataset\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b167c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use function to create dataframe for each dataset\n",
    "\n",
    "caltech_df = metadata_to_df('caltech_images_20210113.json', 'Caltech')\n",
    "island_df = metadata_to_df('island_conservation.json', 'Island')\n",
    "missouri_df = metadata_to_df(\"missouri_camera_traps_set1.json\", 'Missouri')\n",
    "camdeboo_df = metadata_to_df(\"SnapshotCamdeboo_S1_v1.0.json\", 'Camdeboo')\n",
    "wcs_df = metadata_to_df(\"wcs_camera_traps.json\", \"WCS\")\n",
    "ena24_df = metadata_to_df(\"ena24.json\", 'ENA24')\n",
    "wellington_df = metadata_to_df(\"wellington_camera_traps.json\", 'Wellington')\n",
    "karoo_df = metadata_to_df('SnapshotKaroo_S1_v1.0.json', \"Karoo\")\n",
    "kgalagai_df = metadata_to_df('SnapshotKgalagai_S1_v1.0.json', \"Kgalagai\")\n",
    "enonkishu_df = metadata_to_df('SnapshotEnonkishu_S1_v1.0.json', \"Enonkishu\")\n",
    "mountain_zebra_df = metadata_to_df('SnapshotMountainZebra_S1_v1.0.json',\n",
    "                             'Mountain Zebra')\n",
    "kruger_df = metadata_to_df('SnapshotKruger_S1_v1.0.json', 'Kruger')\n",
    "nacti_df = metadata_to_df('nacti_metadata.json', 'NACTI')\n",
    "serengeti_df = metadata_to_df('SnapshotSerengeti_S1-11_v2.1.json',\n",
    "                        'Serengeti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba063c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine resultant dataframes\n",
    "\n",
    "complete_df = pd.concat([caltech_df, island_df, missouri_df,\n",
    "                         camdeboo_df, wcs_df, ena24_df, wellington_df,\n",
    "                         karoo_df, kgalagai_df, enonkishu_df,\n",
    "                         mountain_zebra_df, kruger_df, nacti_df,\n",
    "                         serengeti_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695028c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the corrected species/subfamilies/families information\n",
    "\n",
    "names_df = pd.read_csv(\"names_species_families.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d47b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename column to match with complete_df\n",
    "\n",
    "names_df = names_df.rename(columns={'Label':'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5bf61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge data frames\n",
    "\n",
    "complete_df = complete_df.reset_index().merge(names_df,  how='left', on='name').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f988ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check how many images, species, subfamilies and families are contained in dataframe\n",
    "\n",
    "complete_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1485a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check how many species have at least 1,000 images\n",
    "\n",
    "species_counts = complete_df['Species'].value_counts()\n",
    "highest_species_counts = species_counts[species_counts >= 1000]\n",
    "len(highest_species_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503b041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop species that are either too vague to categorise (e.g. 'Bird') or not animals ('human', 'motorcycle', etc.)\n",
    "\n",
    "highest_species_counts = highest_species_counts.drop(labels=['Bird', 'Car', 'Deer',\n",
    "                            'Domestic animal', 'Human', 'Motorcycle', 'Petrel',\n",
    "                            'Rat', 'Rodent', 'Unknown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4e289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new dataframe of the species that will be classified\n",
    "\n",
    "subset_df = complete_df[complete_df['Species'].isin(highest_species_counts.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fddee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create separate dataframes for each dataset. These will be used to select the images\n",
    "\n",
    "kgalagai_df = subset_df[subset_df[\"Dataset\"] == \"Kgalagai\"]\n",
    "caltech_df = subset_df[subset_df[\"Dataset\"] == \"Caltech\"]\n",
    "island_df = subset_df[subset_df[\"Dataset\"] == \"Island\"]\n",
    "missouri_df = subset_df[subset_df[\"Dataset\"] == \"Missouri\"]\n",
    "camdeboo_df = subset_df[subset_df[\"Dataset\"] == \"Camdeboo\"]\n",
    "wcs_df = subset_df[subset_df[\"Dataset\"] == \"WCS\"]\n",
    "ena24_df = subset_df[subset_df[\"Dataset\"] == \"ENA24\"]\n",
    "wellington_df = subset_df[subset_df[\"Dataset\"] == \"Wellington\"]\n",
    "karoo_df = subset_df[subset_df[\"Dataset\"] == \"Karoo\"]\n",
    "enonkishu_df = subset_df[subset_df[\"Dataset\"] == \"Enonkishu\"]\n",
    "mountain_zebra_df = subset_df[subset_df[\"Dataset\"] == \"Mountain Zebra\"]\n",
    "kruger_df = subset_df[subset_df[\"Dataset\"] == \"Kruger\"]\n",
    "nacti_df = subset_df[subset_df[\"Dataset\"] == \"NACTI\"]\n",
    "serengeti_df = subset_df[subset_df[\"Dataset\"] == \"Serengeti\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d6251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe showing number of species images in each dataset\n",
    "\n",
    "species_per_dataset = subset_df.groupby(['Species', 'Dataset']).size()\n",
    "species_per_dataset = pd.DataFrame(species_per_dataset)\n",
    "species_per_dataset.reset_index(inplace=True)\n",
    "species_per_dataset.columns = ['Species', 'Dataset', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3fde7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove Missouri from dataframe because this will be used as out-of-sample data\n",
    "\n",
    "species_per_dataset = species_per_dataset[~(species_per_dataset['Dataset'] == 'Missouri')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2d112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column which will indicate how many images to use\n",
    "\n",
    "species_per_dataset['Selection'] = species_per_dataset['Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ca2bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change the selection value to maximum of 1,000 divided by the number of datasets containing that species\n",
    "\n",
    "list_of_species = list(species_per_dataset['Species'].unique())\n",
    "for species in list_of_species:\n",
    "    length = len(species_per_dataset[species_per_dataset['Species'] == species])\n",
    "    index = species_per_dataset[species_per_dataset['Species']==species].index\n",
    "    for n in index:\n",
    "        if species_per_dataset['Selection'][n] <= int(1000/length):\n",
    "            continue\n",
    "        else:\n",
    "            species_per_dataset.loc[n,'Selection'] = int(1000/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e25cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column showing the total images selected so far per species, plus column for images left to choose\n",
    "\n",
    "grouped = species_per_dataset.groupby(['Species']).sum()\n",
    "for species in grouped.index:\n",
    "    species_per_dataset.loc[species_per_dataset['Species'] == species, 'Total'] =  grouped.loc[species,'Selection']\n",
    "species_per_dataset['Remaining'] = species_per_dataset['Count'] - species_per_dataset['Selection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7df85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new list based on species where less than 1000 images have been selected\n",
    "new_list_of_species = list(species_per_dataset[species_per_dataset[\"Total\"] != 1000]['Species'].unique())\n",
    "# Repeat above process to select more images\n",
    "for species in new_list_of_species:\n",
    "    length = sum(species_per_dataset[species_per_dataset[\"Remaining\"] != 0]['Species'] == species)\n",
    "    index = species_per_dataset[species_per_dataset[\"Remaining\"] != 0][species_per_dataset[species_per_dataset[\"Remaining\"] != 0]['Species'] == species].index\n",
    "    for n in index:\n",
    "        species_per_dataset.loc[n,'Selection'] += min(int((1000 - species_per_dataset.loc[n, 'Total'])/length), species_per_dataset.loc[n,'Remaining'])\n",
    "for species in list_of_species:\n",
    "    species_per_dataset.loc[species_per_dataset['Species'] == species, 'Total'] =  sum(species_per_dataset.loc[species_per_dataset['Species'] == species, 'Selection'])\n",
    "    species_per_dataset.loc[species_per_dataset[\"Species\"] == species, \"Remaining\"] = species_per_dataset.loc[species_per_dataset[\"Species\"] == species, \"Count\"] - species_per_dataset.loc[species_per_dataset[\"Species\"] == species,\"Selection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2628ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat entire process\n",
    "new_list_of_species = list(species_per_dataset[species_per_dataset[\"Total\"] != 1000]['Species'].unique())\n",
    "for species in new_list_of_species:\n",
    "    length = sum(species_per_dataset[species_per_dataset[\"Remaining\"] != 0]['Species'] == species)\n",
    "    index = species_per_dataset[species_per_dataset[\"Remaining\"] != 0][species_per_dataset[species_per_dataset[\"Remaining\"] != 0]['Species'] == species].index\n",
    "    for n in index:\n",
    "        species_per_dataset.loc[n,'Selection'] += min(int((1000 - species_per_dataset.loc[n, 'Total'])/length), species_per_dataset.loc[n,'Remaining'])\n",
    "for species in list_of_species:\n",
    "    species_per_dataset.loc[species_per_dataset['Species'] == species, 'Total'] =  sum(species_per_dataset.loc[species_per_dataset['Species'] == species, 'Selection'])\n",
    "    species_per_dataset.loc[species_per_dataset[\"Species\"] == species, \"Remaining\"] = species_per_dataset.loc[species_per_dataset[\"Species\"] == species, \"Count\"] - species_per_dataset.loc[species_per_dataset[\"Species\"] == species,\"Selection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55b130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make manual adjustments to final few counts in order to get total to 1,000. \n",
    "\n",
    "species_per_dataset.loc[19,'Selection'] = 148\n",
    "species_per_dataset.loc[20,'Selection'] = 148\n",
    "species_per_dataset.loc[21,'Selection'] = 148\n",
    "species_per_dataset.loc[23,'Selection'] = 148\n",
    "species_per_dataset.loc[36,'Selection'] = 244\n",
    "species_per_dataset.loc[46,'Selection'] = 479\n",
    "species_per_dataset.loc[54,'Selection'] = 251\n",
    "species_per_dataset.loc[69,'Selection'] = 334\n",
    "species_per_dataset.loc[74,'Selection'] = 428\n",
    "species_per_dataset.loc[90,'Selection'] = 355\n",
    "species_per_dataset.loc[100,'Selection'] = 162\n",
    "species_per_dataset.loc[101,'Selection'] = 162\n",
    "species_per_dataset.loc[102,'Selection'] = 162\n",
    "species_per_dataset.loc[104,'Selection'] = 162\n",
    "species_per_dataset.loc[108,'Selection'] = 84\n",
    "species_per_dataset.loc[109,'Selection'] = 84\n",
    "species_per_dataset.loc[110,'Selection'] = 84\n",
    "species_per_dataset.loc[111,'Selection'] = 84\n",
    "species_per_dataset.loc[121,'Selection'] = 216\n",
    "species_per_dataset.loc[123,'Selection'] = 216\n",
    "species_per_dataset.loc[130,'Selection'] = 216\n",
    "species_per_dataset.loc[133,'Selection'] = 306\n",
    "species_per_dataset.loc[134,'Selection'] = 306\n",
    "species_per_dataset.loc[142,'Selection'] = 334\n",
    "species_per_dataset.loc[159,'Selection'] = 202\n",
    "species_per_dataset.loc[160,'Selection'] = 202\n",
    "species_per_dataset.loc[161,'Selection'] = 202\n",
    "species_per_dataset.loc[163,'Selection'] = 195\n",
    "species_per_dataset.loc[165,'Selection'] = 195\n",
    "species_per_dataset.loc[167,'Selection'] = 195\n",
    "species_per_dataset.loc[186,'Selection'] = 178\n",
    "species_per_dataset.loc[188,'Selection'] = 178\n",
    "species_per_dataset.loc[191,'Selection'] = 178\n",
    "species_per_dataset.loc[192,'Selection'] = 178\n",
    "species_per_dataset.loc[195,'Selection'] = 230\n",
    "species_per_dataset.loc[230,'Selection'] = 238\n",
    "species_per_dataset.loc[253,'Selection'] = 427\n",
    "species_per_dataset.loc[255,'Selection'] = 166\n",
    "species_per_dataset.loc[256,'Selection'] = 166\n",
    "species_per_dataset.loc[293,'Selection'] = 334\n",
    "species_per_dataset.loc[312,'Selection'] = 334\n",
    "species_per_dataset.loc[315,'Selection'] = 306\n",
    "species_per_dataset.loc[323,'Selection'] = 267\n",
    "species_per_dataset.loc[326,'Selection'] = 267\n",
    "species_per_dataset.loc[330,'Selection'] = 377\n",
    "species_per_dataset.loc[346,'Selection'] = 143\n",
    "species_per_dataset.loc[347,'Selection'] = 143\n",
    "species_per_dataset.loc[348,'Selection'] = 143\n",
    "species_per_dataset.loc[349,'Selection'] = 143\n",
    "species_per_dataset.loc[350,'Selection'] = 143\n",
    "species_per_dataset.loc[351,'Selection'] = 143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654400a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check that all totals are now 1,000\n",
    "\n",
    "species_per_dataset.groupby(['Species']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ecc99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Second check to confirm we have exactly 1,000 of all species:\n",
    "\n",
    "species_per_dataset.groupby(['Species']).sum()['Selection'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3ab79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to select random images based on the quantities defined in the species_per_dataset dataframe\n",
    "\n",
    "def choose_random_images(dataset, dataframe):\n",
    "    df = pd.DataFrame(columns = dataframe.columns)\n",
    "    for species in species_per_dataset['Species'].unique():\n",
    "        if species in dataframe['Species'].unique():\n",
    "            qty = int(species_per_dataset[(species_per_dataset['Species'] == species)\n",
    "                & (species_per_dataset['Dataset'] == dataset)]['Selection'])\n",
    "            temp_df = (dataframe[dataframe['Species'] == species].sample(qty))\n",
    "            df = pd.concat([df, temp_df])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3127e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new dataframes containing only information on the images to be used in the project\n",
    "\n",
    "df_list = [kgalagai_df, caltech_df, island_df, camdeboo_df, wcs_df, ena24_df, wellington_df, \n",
    "               karoo_df, enonkishu_df, mountain_zebra_df, kruger_df, nacti_df, serengeti_df]\n",
    "name_list = ['Kgalagai', 'Caltech', 'Island', 'Camdeboo', 'WCS', 'ENA 24', 'Wellington', 'Karoo', 'Enonkishu',\n",
    "            'Mountain Zebra', 'Kruger', 'NACTI', 'Serengeti']\n",
    "subset_list = []\n",
    "for i in range(0, len(df_list)):\n",
    "    subset_list.append(choose_random_images(name_list[i], df_list[i]))\n",
    "kgalagai_subset_df, caltech_subset_df, island_subset_df, camdeboo_subset_df, wcs_subset_df, ena24_subset_df, \\\n",
    "wellington_subset_df, karoo_subset_df, enonkishu_subset_df, mountain_zebra_subset_df, kruger_subset_df, \\\n",
    "nacti_subset_df, serengeti_subset_df = subset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f41bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge new dataframes\n",
    "\n",
    "image_selection_df = pd.concat([kgalagai_subset_df, caltech_subset_df,\n",
    "                            island_subset_df, camdeboo_subset_df, wcs_subset_df,\n",
    "                            ena24_subset_df, wellington_subset_df, karoo_subset_df,\n",
    "                            enonkishu_subset_df, mountain_zebra_subset_df,\n",
    "                            kruger_subset_df, nacti_subset_df, serengeti_subset_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3dffdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check length matches expected number (112,000)\n",
    "\n",
    "len(image_selection_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b6b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to pull out the file names to be downloaded (using AzCopy)\n",
    "\n",
    "def images_to_download(dataframe):\n",
    "    x = list(dataframe['file_name'])\n",
    "    x = ';'.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca4392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get lists of images to be downloaded \n",
    "\n",
    "kgalagai_images = images_to_download(kgalagai_subset_df)\n",
    "caltech_images = images_to_download(caltech_subset_df)\n",
    "island_images = images_to_download(island_subset_df)\n",
    "camdeboo_images = images_to_download(camdeboo_subset_df)\n",
    "wcs_images = images_to_download(wcs_dsubset_f)\n",
    "ena24_images = images_to_download(ena24_subset_df)\n",
    "wellington_images = images_to_download(wellington_subset_df)\n",
    "karoo_images = images_to_download(karoo_subset_df)\n",
    "enonkishu_images = images_to_download(enonkishu_subset_df)\n",
    "mountain_zebra_images = images_to_download(mountain_zebra_subset_df)\n",
    "kruger_images = images_to_download(kruger_subset_df)\n",
    "nacti_images = images_to_download(nacti_subset_df)\n",
    "serengeti_images = images_to_download(serengeti_subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c3d63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataframe contains some duplicates (two or more entries for same image). Need to drop these:\n",
    "\n",
    "image_selection_df = image_selection_df.drop_duplicates(subset='file_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58e1cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(image_selection_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1409c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_selection_df['Species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc01a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create training, validation and test datasets (with random choices)\n",
    "\n",
    "def choose_splits(dataframe):\n",
    "    train_df = pd.DataFrame(columns = dataframe.columns)\n",
    "    validation_df = pd.DataFrame(columns = dataframe.columns)\n",
    "    for species in species_list:\n",
    "        temp_df = (dataframe[dataframe['Species'] == species].sample(int(image_selection_df['Species'].value_counts()[species]*.8)))\n",
    "        train_df = pd.concat([train_df, temp_df])\n",
    "    remainder_df = pd.concat([dataframe, train_df]).drop_duplicates(keep=False)\n",
    "    for species in species_list:\n",
    "        temp_df = (remainder_df[remainder_df['Species'] == species].sample(int(image_selection_df['Species'].value_counts()[species]*.1)))\n",
    "        validation_df = pd.concat([validation_df, temp_df])\n",
    "    test_df = pd.concat([remainder_df, validation_df]).drop_duplicates(keep=False)\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc823d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create folders for training, validation and test datasets on local disk\n",
    "\n",
    "path = \"F:\\\\\"\n",
    "training_path = os.path.join(path, \"training\")\n",
    "validation_path = os.path.join(path, \"validation\")\n",
    "test_path = os.path.join(path, \"test\")\n",
    "path_list = [training_path, validation_path, test_path]\n",
    "for path in path_list:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874b6d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of species to be used\n",
    "\n",
    "species_list = list(image_selection_df['Species'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4449de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create folders in each of the training, validation and test folders\n",
    "# One folder for each species in species list\n",
    "\n",
    "train_path = \"F:\\\\training\"\n",
    "validation_path = \"F:\\\\validation\"\n",
    "test_path = \"F:\\\\test\"\n",
    "\n",
    "for species in species_list:\n",
    "    os.makedirs(os.path.join(train_path, species))\n",
    "    os.makedirs(os.path.join(validation_path, species))\n",
    "    os.makedirs(os.path.join(test_path, species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c1b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary mapping datasets to their current location on the local drive\n",
    "\n",
    "path_dict = {'Serengeti': 'F:\\Serengeti\\snapshotserengeti-unzipped',\n",
    "             'WCS':'F:\\WCS\\wcs-unzipped', 'Enonkishu':'F:\\Enonkishu\\ENO_public',\n",
    "             'Camdeboo':'F:\\Camdeboo\\CDB_public', 'Mountain Zebra':'F:\\Mountain Zebra\\MTZ_public',\n",
    "               'Kgalagai':'F:\\Kgalagai\\KGA_public', 'Kruger':'F:\\Kruger\\KRU_public',\n",
    "             'ENA24':'F:\\ENA24\\images', 'Island':'F:\\Island\\public',\n",
    "             'Wellington':'F:\\Wellington\\images', 'Caltech':'F:\\Caltech\\cct_images',\n",
    "               'Karoo':'F:\\Karoo\\KAR_public', \"NACTI\":r\"F:\\NACTI\\nacti-unzipped\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c95c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On inspection, it is clear that files belonging to the WCS dataset often share the same name. The initial\n",
    "# download placed them in different folders (Part 0, Part 1, etc). However, they will potentially be placed\n",
    "# in the same folder when they are arranged into training, validation and test sets. Therefore, it is necessary\n",
    "# to modify the file names in order to avoid overwriting existing files and mixing up labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be75ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Begin by adding a column with a unique number for each file (as a string)\n",
    "\n",
    "image_selection_df['unique_id'] = np.arange(image_selection_df.shape[0])\n",
    "image_selection_df['unique_id'] = image_selection_df['unique_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a335de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add another column to dataframe, this time consisting of the file name without \"jpg\"\n",
    "\n",
    "modification = []\n",
    "for file in image_selection_df['file_name']:\n",
    "        # USe reverse find to work backwards to find start of 'jpg'\n",
    "        index = file.rfind(\".\")\n",
    "        modification.append(file[:index])\n",
    "image_selection_df['file_name_modified'] = modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac87949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the unique number onto the modified file name, plus '.jpg'\n",
    "\n",
    "image_selection_df.loc[image_selection_df[\"Dataset\"] == 'WCS', \n",
    "    \"file_name_modified\"] = image_selection_df.loc[image_selection_df[\"Dataset\"] == 'WCS', \n",
    "            \"file_name_modified\"]+\"_\"+image_selection_df.loc[image_selection_df[\"Dataset\"] == 'WCS', \n",
    "                    \"unique_id\"]+\".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbdd085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe of just the WCS dataset images\n",
    "\n",
    "wcs_new = image_selection_df[image_selection_df['Dataset'] == 'WCS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4efce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the files (using the original file name) and rename using the modified filename\n",
    "\n",
    "for file in wcs_new['file_name']:\n",
    "    old_name = os.path.join(path_dict['WCS'], file)\n",
    "    new_name = os.path.join(path_dict['WCS'], wcs_new[wcs_new['file_name'] == file]['file_name_modified'].iloc[0])\n",
    "    try:\n",
    "        os.rename(old_name, new_name)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6ef26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Can now correct the file name column in the main dataframe\n",
    "\n",
    "image_selection_df.loc[image_selection_df['Dataset'] == 'WCS', \n",
    "        'file_name'] = image_selection_df.loc[image_selection_df['Dataset'] == 'WCS', 'file_name_modified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab7ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the earlier defined function to create the training, validation and test splits\n",
    "\n",
    "train_df, validation_df, test_df = choose_splits(image_selection_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2de8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to copy files from their downloaded location into the correct folder in the\n",
    "# appropriate dataset (training, validation or test)\n",
    "\n",
    "def file_copier(dataframe, path):\n",
    "    for species in training_species:\n",
    "        temp_df = dataframe[dataframe['Species'] == species]\n",
    "        for dataset in temp_df['Dataset'].unique():\n",
    "            temp_df2 = temp_df[temp_df['Dataset'] == dataset]\n",
    "            for file in temp_df2['file_name']:\n",
    "                file_to_locate = os.path.join(path_dict[dataset], file)\n",
    "                new_location = os.path.join(path, species)\n",
    "                try:\n",
    "                    shutil.copy2(file_to_locate, new_location)\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00808d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_copier(train_df, \"F:\\\\training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19e099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_copier(validation_df, \"F:\\\\validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a59f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_copier(test_df, \"F:\\\\test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518c347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can now move on to creating the family-level datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223186b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for and locate any entries without a family value\n",
    "\n",
    "image_selection_df['Species'][image_selection_df['Family'].isna()].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa93d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get index of missing value\n",
    "\n",
    "image_selection_df['Family'][image_selection_df['Species'] == 'Porcupine'][image_selection_df['Family'][image_selection_df['Species'] == 'Porcupine'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500806cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correct missing family value\n",
    "\n",
    "image_selection_df.loc[55942, 'Family'] = 'Hystricidae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a4de6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set subfamily and faily to 'None' for empty images\n",
    "\n",
    "image_selection_df.loc[image_selection_df['Species'] == 'Empty', ['Subfamily', 'Family']] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87350d28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of family values for entries that do not have a subfamily classification\n",
    "\n",
    "families_to_use = list(image_selection_df['Family'][image_selection_df['Subfamily'].isna()].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b621ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add column for one classification (either subfamily or family), intially filled iwth subfamily values\n",
    "\n",
    "image_selection_df['Subfamily/Family'] = image_selection_df['Subfamily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442a5de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change the subfamily/family entry to family classification when subfamily doesn't exist\n",
    "\n",
    "for family in families_to_use:\n",
    "    image_selection_df.loc[image_selection_df['Family'] == family, ['Subfamily/Family']] = family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292b1b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check unique subfamily/family types\n",
    "\n",
    "image_selection_df['Subfamily/Family'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df26a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correct typing error in Reduncinae (extra space at end of word)\n",
    "\n",
    "image_selection_df.loc[image_selection_df['Subfamily/Family'] == 'Reduncinae ', ['Subfamily/Family']] = 'Reduncinae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7b3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save final image selection dataframe\n",
    "\n",
    "image_selection_df.to_csv(\"image_selection_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0627c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create folder to the family-level datasets\n",
    "\n",
    "if not os.path.exists(\"F:\\\\family_data\"):\n",
    "    os.makedirs(\"F:\\\\family_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803d0fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create training, validation and test folders\n",
    "\n",
    "path = \"F:\\\\family_data\"\n",
    "training_path = os.path.join(path, \"training_family\")\n",
    "validation_path = os.path.join(path, \"validation_family\")\n",
    "test_path = os.path.join(path, \"test_family\")\n",
    "path_list = [training_path, validation_path, test_path]\n",
    "for path in path_list:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164a0ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create subfolders in each main folder for every subfamily/family classification\n",
    "\n",
    "families = list(image_selection_df['Subfamily/Family'].unique())\n",
    "train_path = \"F:\\\\family_data\\\\training_family\"\n",
    "validation_path = \"F:\\\\family_data\\\\validation_family\"\n",
    "test_path = \"F:\\\\family_data\\\\test_family\"\n",
    "\n",
    "for family in families:\n",
    "    os.makedirs(os.path.join(train_path, family))\n",
    "    os.makedirs(os.path.join(validation_path, family))\n",
    "    os.makedirs(os.path.join(test_path, family))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a146cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create random training, validation and test splits\n",
    "\n",
    "def choose_splits_family(dataframe):\n",
    "    train_df = pd.DataFrame(columns = dataframe.columns)\n",
    "    validation_df = pd.DataFrame(columns = dataframe.columns)\n",
    "    for family in families:\n",
    "        temp_df = (dataframe[dataframe['Subfamily/Family'] == family].sample(int(dataframe['Subfamily/Family'].value_counts()[family]*.8)))\n",
    "        train_df = pd.concat([train_df, temp_df])\n",
    "    remainder_df = pd.concat([dataframe, train_df]).drop_duplicates(keep=False)\n",
    "    for family in families:\n",
    "        temp_df = (remainder_df[remainder_df['Subfamily/Family'] == family].sample(int(dataframe['Subfamily/Family'].value_counts()[family]*.1)))\n",
    "        validation_df = pd.concat([validation_df, temp_df])\n",
    "    test_df = pd.concat([remainder_df, validation_df]).drop_duplicates(keep=False)\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2a906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the splits\n",
    "\n",
    "family_train_df, family_validation_df, family_test_df = choose_splits_family(image_selection_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa3bfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to copy files from original download location to approrpiate family-level folder\n",
    "\n",
    "def file_copier_family(dataframe, path):\n",
    "    for family in families:\n",
    "        temp_df = dataframe[dataframe['Subfamily/Family'] == family]\n",
    "        for dataset in temp_df['Dataset'].unique():\n",
    "            temp_df2 = temp_df[temp_df['Dataset'] == dataset]\n",
    "            for file in temp_df2['file_name']:\n",
    "                file_to_locate = os.path.join(path_dict[dataset], file)\n",
    "                new_location = os.path.join(path, family)\n",
    "                try:\n",
    "                    shutil.copy2(file_to_locate, new_location)\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ecd132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_copier_family(family_validation_df, \"F:\\\\family_data\\\\validation_family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8f4d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_copier_family(family_test_df, \"F:\\\\family_data\\\\test_family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79602f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_copier_family(family_train_df, \"F:\\\\family_data\\\\training_family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77964060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import credentials for GCP bucket\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"x-pathway-318914-d2e75ae928d4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4be311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define GCP bucket\n",
    "\n",
    "client=storage.Client()\n",
    "bucket = client.get_bucket(\"dsm500_bucket_europe_west2_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18908494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create function to upload images from local disk to appropriate folder in GCP bucket\n",
    "\n",
    "def upload_local_directory_to_gcs(upload_list, location, bucket):\n",
    "    for item in upload_list:\n",
    "        local_path = \"F://\" + location + \"/\" + item\n",
    "        gcs_path = location + \"/\" + item\n",
    "        assert os.path.isdir(local_path)\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_gcs(local_file, bucket, gcs_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = gcs_path + \"/\" + local_file[1 + len(local_path):]\n",
    "                blob = bucket.blob(remote_path)\n",
    "                blob.upload_from_filename(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745481a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs(species_list, \"training\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb6878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs(species_list, \"validation\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc57ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs(species_list, \"test\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d537d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create list of subfamily/family classifications to use in uploading function\n",
    "\n",
    "family_list = list(image_selection_df['Subfamily/Family'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bcdbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs(family_list, \"family_data/training_family\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee8710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs(family_list, \"family_data/validation_family\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6c9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs(family_list, \"family_data/test_family\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model training attempt required too much computational power. Therefore, decision was made to\n",
    "# delete any subfamilies/families with only one species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33486390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of families/subfamilies with less than/equal to 1,000 images (indicating presence of only one species) \n",
    "\n",
    "families_to_delete = list((image_selection_df.groupby('Subfamily/Family').size()[image_selection_df.groupby('Subfamily/Family').size() <=1000]).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb0159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use families list to get species images to delete \n",
    "species_to_delete = []\n",
    "for family in families_to_delete:\n",
    "       species_to_delete.append(image_selection_df[image_selection_df['Subfamily/Family'] == family]['Species'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf71c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, need to get extra unused images to be used for the species_level classifier test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1a270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe of unused images, then filter it to contain only Canidae, Felinae or Sciruidae images\n",
    "\n",
    "used_files = list(image_selection_df['file_name'])\n",
    "unused_images_df = subset_df[~subset_df['id'].isin(used_files)]\n",
    "unused_images_df = unused_images_df.drop_duplicates(subset='id')\n",
    "# Use 'felidae' in the or statement in order to filter by family column\n",
    "unused_images_df = unused_images_df.loc[(unused_images_df['Family'] == 'Felidae') | (unused_images_df['Family'] == 'Sciuridae') | (unused_images_df['Family'] == 'Canidae')]\n",
    "# Remove pantherinae in order to be left with felinae\n",
    "unused_images_df = unused_images_df[~(unused_images_df['Subfamily'] == 'Pantherinae')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1393c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repeat process performed above to get proprotoinate number of images from each different dataset\n",
    "\n",
    "species_per_dataset = unused_images_df.groupby(['Species', 'Dataset']).size()\n",
    "species_per_dataset = pd.DataFrame(species_per_dataset)\n",
    "species_per_dataset.reset_index(inplace=True)\n",
    "species_per_dataset.columns = ['Species', 'Dataset', 'Count']\n",
    "species_per_dataset = species_per_dataset[~(species_per_dataset['Dataset'] == 'Missouri')]\n",
    "species_per_dataset['Selection'] = species_per_dataset['Count']\n",
    "list_of_species = list(species_per_dataset['Species'].unique())\n",
    "for species in list_of_species:\n",
    "    length = len(species_per_dataset[species_per_dataset['Species'] == species])\n",
    "    index = species_per_dataset[species_per_dataset['Species']==species].index\n",
    "    for n in index:\n",
    "        if species_per_dataset['Selection'][n] <= int(100/length):\n",
    "            continue\n",
    "        else:\n",
    "            species_per_dataset.loc[n,'Selection'] = int(100/length)\n",
    "grouped = species_per_dataset.groupby(['Species']).sum()\n",
    "for species in grouped.index:\n",
    "    species_per_dataset.loc[species_per_dataset['Species'] == species, 'Total'] =  grouped.loc[species,'Selection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da8774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manual correction of one row\n",
    "\n",
    "species_per_dataset.loc[38,'Selection'] = 50+20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88b31c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm selectin includes 100 images for each species\n",
    "\n",
    "species_per_dataset.groupby(['Species']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52ad27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new dataframes of unused images by dataset\n",
    "\n",
    "kgalagai_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Kgalagai\"]\n",
    "caltech_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Caltech\"]\n",
    "island_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Island\"]\n",
    "missouri_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Missouri\"]\n",
    "camdeboo_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Camdeboo\"]\n",
    "wcs_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"WCS\"]\n",
    "ena24_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"ENA24\"]\n",
    "wellington_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Wellington\"]\n",
    "karoo_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Karoo\"]\n",
    "enonkishu_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Enonkishu\"]\n",
    "mountain_zebra_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Mountain Zebra\"]\n",
    "kruger_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Kruger\"]\n",
    "nacti_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"NACTI\"]\n",
    "serengeti_new_df = unused_images_df[unused_images_df[\"Dataset\"] == \"Serengeti\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537abcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use new dataframes and image selection function to get dataframe of new test images\n",
    "\n",
    "kgalagai_new_test = choose_random_images('Kgalagai', kgalagai_new_df)\n",
    "caltech_new_test = choose_random_images('Caltech', caltech_new_df)\n",
    "island_new_test = choose_random_images('Island', island_new_df)\n",
    "camdeboo_new_test = choose_random_images('Camdeboo', camdeboo_new_df)\n",
    "wcs_new_test = choose_random_images('WCS', wcs_new_df)\n",
    "ena24_new_test = choose_random_images('ENA24', ena24_new_df)\n",
    "wellington_new_test = choose_random_images('Wellington', wellington_new_df)\n",
    "karoo_new_test = choose_random_images('Karoo', karoo_new_df)\n",
    "enonkishu_new_test = choose_random_images('Enonkishu', enonkishu_new_df)\n",
    "mountain_new_test = choose_random_images('Mountain Zebra', mountain_zebra_new_df)\n",
    "kruger_new_test = choose_random_images('Kruger', kruger_new_df)\n",
    "nacti_new_test = choose_random_images('NACTI', nacti_new_df)\n",
    "serengeti_new_test = choose_random_images('Serengeti', serengeti_new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc65bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combbine into single dataframe\n",
    "\n",
    "new_image_selection_df = pd.concat([kgalagai_new_test, caltech_new_test,\n",
    "                            island_new_test, camdeboo_new_test, wcs_new_test,\n",
    "                            ena24_new_test, wellington_new_test,\n",
    "                            karoo_new_test, enonkishu_new_test,\n",
    "                            mountain_new_test, kruger_new_test,\n",
    "                            nacti_new_test, serengeti_new_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d82bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use eariler function to get download information for use with AzCopy\n",
    "\n",
    "kgalagai_new_images = images_to_download('Kgalagai', kgalagai_new_test)\n",
    "caltech_new_images = images_to_download('Caltech', caltech_new_test)\n",
    "island_new_images = images_to_download('Island', island_new_test)\n",
    "camdeboo_new_images = images_to_download('Camdeboo', camdeboo_new_test)\n",
    "wcs_new_images = images_to_download('WCS', wcs_new_test)\n",
    "ena24_new_images = images_to_download('ENA24', ena24_new_test)\n",
    "wellington_new_images = images_to_download('Wellington', wellington_new_test)\n",
    "karoo_new_images = images_to_download('Karoo', karoo_new_test)\n",
    "enonkishu_new_images = images_to_download('Enonkishu', enonkishu_new_test)\n",
    "mountain_zebra_new_images = images_to_download('Mountain Zebra', mountain_new_test)\n",
    "kruger_new_images = images_to_download('Kruger', kruger_new_test)\n",
    "nacti_new_images = images_to_download('NACTI', nacti_new_test)\n",
    "serengeti_new_images = images_to_download('Serengeti', serengeti_new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b828b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define new mapping and species list for copying files to correct location\n",
    "\n",
    "path_dict_new = {'Serengeti': 'F:\\Final_Test_Data\\Serengeti\\snapshotserengeti-unzipped',\n",
    "             'WCS':'F:\\Final_Test_Data\\WCS\\wcs-unzipped',\n",
    "             'Camdeboo':'F:\\Final_Test_Data\\Camdeboo\\CDB_public', \n",
    "                 'Mountain Zebra':'F:\\Final_Test_Data\\Mountain Zebra\\MTZ_public',\n",
    "               'Kgalagai':'F:\\Final_Test_Data\\Kgalagai\\KGA_public', \n",
    "                 'Kruger':'F:\\Final_Test_Data\\Kruger\\KRU_public',\n",
    "             'ENA24':'F:\\Final_Test_Data\\ENA24\\images', \n",
    "                 'Island':'F:\\Final_Test_Data\\Island\\public',\n",
    "             'Wellington':'F:\\Final_Test_Data\\Wellington\\images', \n",
    "                 'Caltech':'F:\\Final_Test_Data\\Caltech\\cct_images',\n",
    "               'Karoo':'F:\\Final_Test_Data\\Karoo\\KAR_public', \n",
    "                 \"NACTI\":r\"F:\\Final_Test_Data\\NACTI\\nacti-unzipped\"}\n",
    "\n",
    "new_species_list = list(new_image_selection_df['Species'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad2cc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modified file copier function\n",
    "\n",
    "def file_copier(dataframe, path):\n",
    "    for species in new_species_list:\n",
    "        temp_df = dataframe[dataframe['Species'] == species]\n",
    "        for dataset in temp_df['Dataset'].unique():\n",
    "            temp_df2 = temp_df[temp_df['Dataset'] == dataset]\n",
    "            for file in temp_df2['file_name']:\n",
    "                file_to_locate = os.path.join(path_dict_new[dataset], file)\n",
    "                new_location = os.path.join(path, species)\n",
    "                try:\n",
    "                    shutil.copy2(file_to_locate, new_location)\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8c00e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy files\n",
    "\n",
    "file_copier(new_image_selection_df, \"F:\\\\Final_Test_Data\\Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9746bd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create lists for uploading to CGP\n",
    "\n",
    "felinae_list = list(new_image_selection_df[new_image_selection_df['Subfamily'] == 'Felinae']['Species'].unique())\n",
    "canidae_list = list(new_image_selection_df[new_image_selection_df['Family'] == 'Canidae']['Species'].unique())\n",
    "sciuridae_list = list(new_image_selection_df[new_image_selection_df['Family'] == 'Sciuridae']['Species'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990aec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs(felinae_list, \"felinae/test\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5956cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs(canidae_list, \"canidae/test\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06821f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs(sciuridae_list, \"sciuridae/test\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552e83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now need to upload out-of-sample data using Missouri dataset. Entire dataset was previously downloaded in bulk,\n",
    "# so simply a case of selecting approrpiate images and uploading to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc137021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe of appropriate images (canidae, felinae, sciuridae)\n",
    "\n",
    "missouri_df = unused_images_df[unused_images_df['Dataset']=='Missouri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_local_directory_to_gcs(upload_list, location, bucket):\n",
    "    for item in upload_list:\n",
    "        local_path = \"F://\" + location + \"/\" + item\n",
    "        gcs_path = location + \"/\" + item\n",
    "        assert os.path.isdir(local_path)\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_gcs(local_file, bucket, gcs_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = gcs_path + \"/\" + local_file[1 + len(local_path):]\n",
    "                blob = bucket.blob(remote_path)\n",
    "                blob.upload_from_filename(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb847fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simplified uploaded function to account for fact that only one species present per class (so list not used)\n",
    "\n",
    "def upload_local_directory_to_gcs2(item, location, bucket):\n",
    "    local_path = \"F://\" + location + \"/\" + item\n",
    "    gcs_path = location + \"/\" + item\n",
    "    assert os.path.isdir(local_path)\n",
    "    for local_file in glob.glob(local_path + '/**'):\n",
    "        if not os.path.isfile(local_file):\n",
    "            upload_local_directory_to_gcs(local_file, bucket, gcs_path + \"/\" + os.path.basename(local_file))\n",
    "        else:\n",
    "            remote_path = gcs_path + \"/\" + local_file[1 + len(local_path):]\n",
    "            blob = bucket.blob(remote_path)\n",
    "            blob.upload_from_filename(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e57eb24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Locate in species list and upload the specific species present in missouri dataset\n",
    "\n",
    "upload_local_directory_to_gcs2(canidae_list[2], \"canidae/out_of_sample\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d92d5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs2(sciuridae_list[1], \"sciuridae/out_of_sample\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76269a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upload_local_directory_to_gcs2(felinae_list[3], \"felinae/out_of_sample\", bucket)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
