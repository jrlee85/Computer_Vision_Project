{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05004f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define font to be used for graphics and increase dataframe rows visible in window\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2450ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read metrics from json files\n",
    "\n",
    "def json_reader(file):\n",
    "    with open(file) as json_data:\n",
    "        json_dict = json.load(json_data)\n",
    "    return json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5eeca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of metrics for each model\n",
    "\n",
    "benchmark_training_df = pd.DataFrame.from_dict(json_reader('benchmark.json'))\n",
    "family_training_df = pd.DataFrame.from_dict(json_reader('family.json'))\n",
    "canidae_training_df = pd.DataFrame.from_dict(json_reader('canidae.json'))\n",
    "felinae_training_df = pd.DataFrame.from_dict(json_reader('felinae.json'))\n",
    "sciuridae_training_df = pd.DataFrame.from_dict(json_reader('sciuridae.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130156e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up dataframe\n",
    "\n",
    "def dataframe_tidy(df):\n",
    "    df.index = range(1,51)\n",
    "    df.index.name='Epoch'\n",
    "    df.columns = [\"Training Loss\", \"Training Accuracy\", \"Top 5 Training Accuracy\", \n",
    "                                 \"Validation Loss\", \"Validation Accuracy\", \"Validation Top 5 Accuracy\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_training_df = dataframe_tidy(benchmark_training_df)\n",
    "family_training_df = dataframe_tidy(family_training_df)\n",
    "canidae_training_df = dataframe_tidy(canidae_training_df)\n",
    "felinae_training_df = dataframe_tidy(felinae_training_df)\n",
    "sciuridae_training_df = dataframe_tidy(sciuridae_training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30209012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as images for use in report\n",
    "\n",
    "dfi.export(benchmark_training_df, 'results_images/benchmark_training_df.png')\n",
    "dfi.export(family_training_df, 'results_images/family_training_df.png')\n",
    "dfi.export(canidae_training_df, 'results_images/canidae_training_df.png')\n",
    "dfi.export(felinae_training_df, 'results_images/felinae_training_df.png')\n",
    "dfi.export(sciuridae_training_df, 'results_images/sciuridae_training_df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ec3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training/validation metrics for best epoch for each model\n",
    "\n",
    "benchmark_best_epoch = pd.DataFrame(benchmark_training_df.loc[48,]).transpose()\n",
    "family_best_epoch = pd.DataFrame(family_training_df.loc[45,]).transpose()\n",
    "canidae_best_epoch = pd.DataFrame(canidae_training_df.loc[43,]).transpose()\n",
    "felinae_best_epoch = pd.DataFrame(felinae_training_df.loc[40,]).transpose()\n",
    "sciuridae_best_epoch = pd.DataFrame(sciuridae_training_df.loc[35,]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4186035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into single dataframe for visualisation\n",
    "\n",
    "best_epoch_df = pd.concat([benchmark_best_epoch, family_best_epoch,\n",
    "                          canidae_best_epoch, felinae_best_epoch,\n",
    "                          sciuridae_best_epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy and save dataframe\n",
    "\n",
    "best_epoch_df.reset_index(inplace=True)\n",
    "best_epoch_df.rename(columns={'index':'Epoch'}, inplace=True)\n",
    "best_epoch_df.index = ['Benchmark', 'Family-Level', 'Canidae',\n",
    "                       'Felinae', 'Sciuridae']\n",
    "best_epoch_df.index.name = 'Classifier'\n",
    "best_epoch_df = best_epoch_df.round(2)\n",
    "dfi.export(best_epoch_df, 'results_images/best_epoch_df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training/validation progress\n",
    "\n",
    "def train_val_plot(df, best_epoch, acc_text_y, acc_arrow_y, val_text_y, val_arrow_y, filename):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    fig = plt.figure(figsize=(15.5,5.5))\n",
    "    N = np.arange(1, len(df)+1)\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.plot(N, df[\"Training Accuracy\"], label=\"Training Accuracy\")\n",
    "    ax1.plot(N, df[\"Validation Accuracy\"], label=\"Validation Accuracy\")\n",
    "    ax1.set_title(\"Training and Validation Accuracy\", pad=20, size=18)\n",
    "    ax1.set_xlabel(\"Epoch\", size=14)\n",
    "    ax1.set_ylabel(\"Accuracy\", size=14)\n",
    "    ax1.axvline(x=best_epoch, color='k', linestyle = 'dashed', linewidth = 0.5)\n",
    "    ax1.annotate('Epoch used for evaluation', xy=(best_epoch-23, acc_text_y), size=12)\n",
    "    ax1.arrow(x=best_epoch-13, y=acc_arrow_y, dx=12, dy=0, color='k', head_width=0.02,\n",
    "              width=0.005, head_length=0.4, alpha=0.5) \n",
    "    ax1.legend(fontsize=12)\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.plot(N, benchmark_training_df[\"Training Loss\"], label=\"Training Loss\")\n",
    "    ax2.plot(N, benchmark_training_df[\"Validation Loss\"], label=\"Validation Loss\")\n",
    "    ax2.set_title(\"Training and Validation Loss\", pad=20, size=18)\n",
    "    ax2.set_xlabel(\"Epoch\", size=14)\n",
    "    ax2.set_ylabel(\"Loss\", size=14)\n",
    "    ax2.axvline(x=best_epoch, color='k', linestyle = 'dashed', linewidth = 0.5)\n",
    "    ax2.annotate('Epoch used for evaluation', xy=(best_epoch-23, val_text_y), size=12)\n",
    "    ax2.arrow(x=best_epoch-13, y=val_arrow_y, dx=12, dy=0, color='k', head_width=0.1,\n",
    "              width=0.025, head_length=0.4, alpha=0.5) \n",
    "    ax2.legend(fontsize=12)\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top = None, wspace=None, hspace = 0.3)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save as image file training/validation progress\n",
    "\n",
    "train_val_plot(benchmark_training_df, 48, 0.55, 0.53, 2.25, 2.15, \"results_images/benchmark_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_plot(family_training_df, 45, 0.55, 0.53, 2.25, 2.15, \"results_images/family_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493fb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_plot(canidae_training_df, 43, 0.55, 0.53, 2.25, 2.15, \"results_images/canidae_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b58e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_plot(felinae_training_df, 40, 0.55, 0.53, 2.25, 2.15, \"results_images/felinae_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_plot(sciuridae_training_df, 35, 0.925, 0.915, 2.25, 2.15, \"results_images/sciuridae_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dataframe showing test performance by model\n",
    "\n",
    "def test_scores(file_list):\n",
    "    df = pd.read_csv(file_list[0])\n",
    "    for file in file_list[1:]:\n",
    "        test_scores_df = pd.concat([df,pd.read_csv(file)])\n",
    "        df = test_scores_df\n",
    "    test_scores_df.drop(columns='Unnamed: 0', inplace=True)\n",
    "    test_scores_df.index = ['Benchmark', 'Family-Level', 'Canidae', \n",
    "                            'Felinae', 'Sciuridae']\n",
    "    test_scores_df.index.name = 'Classifier'\n",
    "    test_scores_df = test_scores_df.round(2)\n",
    "    return test_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50451eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_df = test_scores([\"./Results/output_benchmark_score.csv\", \"./Results/family_data_output_score.csv\",\n",
    "                   \"./Results/canidae_output_canidae_score.csv\",\"./Results/felinae_output_felinae_score.csv\",\n",
    "                   \"./Results/sciuridae_output_sciuridae_score.csv\"])\n",
    "dfi.export(test_scores_df, 'results_images/test_scores_df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e30c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot test performance\n",
    "\n",
    "def test_plot(df, filename):\n",
    "    df_melted = df.drop(columns='Loss')\n",
    "    df_melted = df_melted.reset_index()\n",
    "    df_melted = pd.melt(df_melted, id_vars='Classifier')\n",
    "    df_melted.columns = ['Classifier', 'Metric', 'Accuracy']\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    ax = sns.barplot(y='Classifier', x='Accuracy', hue='Metric', data=df_melted, orient='h', palette='deep')\n",
    "    ax.set_title('Test Accuracy', pad=20, size=18)\n",
    "    ax.set_xlim(0, 1.3)\n",
    "    ax.set_xlabel('Accuracy', size=14, weight='bold')\n",
    "    ax.set_ylabel('Classifier', size=14, weight='bold')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, padding=5, fmt='%.2f', size=11)\n",
    "    ax.legend(fontsize=12)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27227cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plot(test_scores_df, \"results_images/test_scores_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086fdaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create two dataframes: (1) precision, recall and F1 score by species (2) weighted average\n",
    "# across all species\n",
    "\n",
    "def test_evaluation(file, index_label, name):\n",
    "    df = pd.read_csv(file)\n",
    "    df.columns=[index_label, \"Precision\", \"Recall\", \"F1-Score\", \"No. Images\"]\n",
    "    df.set_index(index_label, inplace=True)\n",
    "    df = df.round(2)\n",
    "    df2 = df.tail(1)\n",
    "    df2.index=[name]\n",
    "    df2.index.name = 'Classifier'\n",
    "    df.drop(df.tail(3).index,inplace = True)\n",
    "    return df, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7223708",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_test_evaluation_df, benchmark_avg = test_evaluation(\"./Results/output_benchmark_evaluation.csv\", \n",
    "                                                              'Species', 'Benchmark')\n",
    "dfi.export(benchmark_test_evaluation_df, 'results_images/benchmark_test_evaluation_df.png')\n",
    "family_test_evaluation_df, family_avg = test_evaluation(\"./Results/family_data_output_evaluation.csv\",\n",
    "                                                        'Family', 'Family')\n",
    "dfi.export(family_test_evaluation_df, 'results_images/family_test_evaluation_df.png')\n",
    "canidae_test_evaluation_df, canidae_avg = test_evaluation(\"./Results/canidae_output_canidae_evaluation.csv\",\n",
    "                                                          'Species', 'Canidae')\n",
    "dfi.export(canidae_test_evaluation_df, 'results_images/canidae_test_evaluation_df.png')\n",
    "felinae_test_evaluation_df, felinae_avg = test_evaluation(\"./Results/felinae_output_felinae_evaluation.csv\",\n",
    "                                                          'Species', 'Felinae')\n",
    "dfi.export(felinae_test_evaluation_df, 'results_images/felinae_test_evaluation_df.png')\n",
    "sciuridae_test_evaluation_df, sciuridae_avg = test_evaluation(\"./Results/sciuridae_output_sciuridae_evaluation.csv\",\n",
    "                                                              'Species', 'Sciuridae')\n",
    "dfi.export(sciuridae_test_evaluation_df, 'results_images/sciuridae_test_evaluation_df.png')\n",
    "weighted_average_df = pd.concat([benchmark_avg, family_avg,\n",
    "                               canidae_avg, felinae_avg, sciuridae_avg])\n",
    "dfi.export(weighted_average_df, 'results_images/weighted_average_df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot weighted average metrics\n",
    "\n",
    "def weighted_average_plot(df, filename):\n",
    "    df_melted = df.drop(columns='No. Images')\n",
    "    df_melted = df_melted.reset_index()\n",
    "    df_melted = pd.melt(df_melted, id_vars='Classifier')\n",
    "    df_melted.columns = ['Classifier', 'Metric', 'Value']\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    ax = sns.barplot(y='Classifier', x='Value', hue='Metric', data=df_melted, orient='h', palette='deep')\n",
    "    ax.set_title('Test Evaluation', pad=20, size=18)\n",
    "    ax.set_xlim(0, 1.2)\n",
    "    ax.set_xlabel('Value', size=14, weight='bold')\n",
    "    ax.set_ylabel('Classifier', size=14, weight='bold')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, padding=5, fmt='%.2f', size=11)\n",
    "    ax.legend(fontsize=12)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fa46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average_plot(weighted_average_df, \"results_images/weighted_average_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create mappings between numeric labels and equivalent species name\n",
    "\n",
    "def create_mappings(df):\n",
    "    mapping={}\n",
    "    for i in range(0,len(df)):\n",
    "        mapping[i] = df.index[i]\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create:\n",
    "# (1) dataframe of incorrect classifications\n",
    "# (2) dataframe showing filenames of incorrect classifications\n",
    "# (3) dataframe showing filenames of correct classifications\n",
    "# (4) confusion matrix for predictions vs. truth\n",
    "\n",
    "def test_mistakes(file, mapping):\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    # reorder columns\n",
    "    df = df[[\"Actual\", \"Prediction\", \"File\"]]\n",
    "    # Change integers to labels\n",
    "    df['Prediction']=df['Prediction'].map(mapping)\n",
    "    df['Actual']=df['Actual'].map(mapping)\n",
    "    # Create separate dataframe of misclassified images\n",
    "    misclassified_images = df[df[\"Prediction\"] != df[\"Actual\"]]\n",
    "    # Create separate dataframe of correct classifications\n",
    "    correct_classifications = df[df[\"Prediction\"] == df[\"Actual\"]]\n",
    "    # Summarise mislcassifications and create confusion matrix\n",
    "    df.drop(columns='File', inplace=True)\n",
    "    conf_mat = confusion_matrix(df['Actual'], df['Prediction'])\n",
    "    df = df.groupby(['Actual', 'Prediction']).size().reset_index()\n",
    "    df.rename(columns={0:'Count'}, inplace=True)\n",
    "    return df, misclassified_images, correct_classifications, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save labelled confusion matrix \n",
    "\n",
    "def conf_matrix(matrix, labels, filename):\n",
    "    sns.set(font_scale=1.2)\n",
    "    plt.figure(figsize = (10,7))  \n",
    "    cf = sns.heatmap(matrix, cmap='Blues', annot=True, cbar=False, \n",
    "            xticklabels = labels,  yticklabels = labels, fmt='g')   \n",
    "    cf.set_xticklabels(cf.get_xticklabels(), rotation=90) \n",
    "    cf.set_yticklabels(cf.get_yticklabels(), rotation=0)\n",
    "    cf.set_title('Predicted vs. Actual Class', pad=30, size=18)\n",
    "    plt.xlabel(\"Prediction\", weight='bold')\n",
    "    plt.ylabel(\"Actual\", weight='bold')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mapping = create_mappings(benchmark_test_evaluation_df)\n",
    "benchmark_mistakes_df, benchmark_mistake_files, benchmark_correct, benchmark_conf_mat = test_mistakes(\"./Results/output_benchmark_mistakes.csv\", \n",
    "                                                                                  benchmark_mapping)\n",
    "conf_matrix(benchmark_conf_mat, list(benchmark_test_evaluation_df.index), \"results_images/benchmark_conf_mat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e97677",
   "metadata": {},
   "outputs": [],
   "source": [
    "canidae_mapping = create_mappings(canidae_test_evaluation_df)\n",
    "canidae_mistakes_df, canidae_mistake_files, canidae_correct, canidae_conf_mat = test_mistakes(\"./Results/canidae_output_canidae_mistakes.csv\", \n",
    "                                                                            canidae_mapping)\n",
    "dfi.export(canidae_mistakes_df, 'results_images/canidae_mistakes_df.png')\n",
    "conf_matrix(canidae_conf_mat, list(canidae_test_evaluation_df.index), \"results_images/canidae_conf_mat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c67a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "felinae_mapping = create_mappings(felinae_test_evaluation_df)\n",
    "felinae_mistakes_df, felinae_mistake_files, felinae_correct, felinae_conf_mat = test_mistakes(\"./Results/felinae_output_felinae_mistakes.csv\", \n",
    "                                                                            felinae_mapping)\n",
    "dfi.export(felinae_mistakes_df, 'results_images/felinae_mistakes_df.png')\n",
    "conf_matrix(felinae_conf_mat, list(felinae_test_evaluation_df.index), \"results_images/felinae_conf_mat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciuridae_mapping = create_mappings(sciuridae_test_evaluation_df)\n",
    "sciuridae_mistakes_df, sciuridae_mistake_files, sciuridae_correct, sciuridae_conf_mat = test_mistakes(\"./Results/sciuridae_output_sciuridae_mistakes.csv\",\n",
    "                                                                                  sciuridae_mapping)\n",
    "dfi.export(sciuridae_mistakes_df, 'results_images/sciuridae_mistakes_df.png')\n",
    "conf_matrix(sciuridae_conf_mat, list(sciuridae_test_evaluation_df.index), \"results_images/sciuridae_conf_mat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264197b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat entire test process for out-of-sample datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def oos_scores(file_list):\n",
    "    df = pd.read_csv(file_list[0])\n",
    "    for file in file_list[1:]:\n",
    "        oos_scores_df = pd.concat([df,pd.read_csv(file)])\n",
    "        df = oos_scores_df\n",
    "    oos_scores_df.drop(columns='Unnamed: 0', inplace=True)\n",
    "    oos_scores_df.index = ['Benchmark', 'Canidae', \n",
    "                            'Felinae', 'Sciuridae']\n",
    "    oos_scores_df.index.name = 'Classifier'\n",
    "    return oos_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_scores_df = oos_scores([\"./Results/output_benchmark_oos_score.csv\",\n",
    "                   \"./Results/canidae_output_canidae_oos_score.csv\",\n",
    "                   \"./Results/felinae_output_felinae_oos_score.csv\",\n",
    "                   \"./Results/sciuridae_output_sciuridae_oos_score.csv\"])\n",
    "dfi.export(oos_scores_df, 'results_images/oos_scores_df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35937df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot out-of-sample scores\n",
    "\n",
    "def oos_plot(df, filename):\n",
    "    df_melted = df.drop(columns='Loss')\n",
    "    df_melted = df_melted.reset_index()\n",
    "    df_melted = pd.melt(df_melted, id_vars='Classifier')\n",
    "    df_melted.columns = ['Classifier', 'Metric', 'Accuracy']\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    ax = sns.barplot(y='Classifier', x='Accuracy', hue='Metric', data=df_melted, orient='h', palette='deep')\n",
    "    ax.set_title('Out-of-Sample Accuracy', pad=20, size=18)\n",
    "    ax.set_xlim(0, 1.3)\n",
    "    ax.set_xlabel('Accuracy', size=14, weight='bold')\n",
    "    ax.set_ylabel('Classifier', size=14, weight='bold')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, padding=5, fmt='%.2f', size=11)\n",
    "    ax.legend(fontsize=12)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da234cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_plot(oos_scores_df, \"results_images/oos_scores_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af52736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test function to create equivalent dataframes for out-of-sample data\n",
    "\n",
    "benchmark_oos_evaluation_df, benchmark_oos_avg = test_evaluation(\"./Results/output_benchmark_oos_evaluation.csv\", \n",
    "                                                              'Species', 'Benchmark')\n",
    "canidae_oos_evaluation_df, canidae_oos_avg = test_evaluation(\"./Results/canidae_output_canidae_oos_evaluation.csv\",\n",
    "                                                          'Species', 'Canidae')\n",
    "felinae_oos_evaluation_df, felinae_oos_avg = test_evaluation(\"./Results/felinae_output_felinae_oos_evaluation.csv\",\n",
    "                                                          'Species', 'Felinae')\n",
    "sciuridae_oos_evaluation_df, sciuridae_oos_avg = test_evaluation(\"./Results/sciuridae_output_sciuridae_oos_evaluation.csv\",\n",
    "                                                              'Species', 'Sciuridae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57eccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(benchmark_oos_evaluation_df, 'results_images/benchmark_oos_evaluation_df.png')\n",
    "dfi.export(canidae_oos_evaluation_df, 'results_images/canidae_oos_evaluation_df.png')\n",
    "dfi.export(felinae_oos_evaluation_df, 'results_images/felinae_oos_evaluation_df.png')\n",
    "dfi.export(sciuridae_oos_evaluation_df, 'results_images/sciuridae_oos_evaluation_df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average_oos_df = pd.concat([benchmark_oos_avg, canidae_oos_avg,\n",
    "                                     felinae_oos_avg, sciuridae_oos_avg])\n",
    "dfi.export(weighted_average_oos_df, 'results_images/weighted_average_oos_df.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d24830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_oos_plot(df, filename):\n",
    "    df_melted = df.drop(columns='No. Images')\n",
    "    df_melted = df_melted.reset_index()\n",
    "    df_melted = pd.melt(df_melted, id_vars='Classifier')\n",
    "    df_melted.columns = ['Classifier', 'Metric', 'Value']\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    ax = sns.barplot(y='Classifier', x='Value', hue='Metric', data=df_melted, orient='h', palette='deep')\n",
    "    ax.set_title('Out-of-Sample Evaluation', pad=20, size=18)\n",
    "    ax.set_xlim(0, 1.3)\n",
    "    ax.set_xlabel('Value', size=14, weight='bold')\n",
    "    ax.set_ylabel('Classifier', size=14, weight='bold')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, padding=5, fmt='%.2f', size=11)\n",
    "    ax.legend(fontsize=12)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average_oos_plot(weighted_average_oos_df, \"results_images/weighted_average_oos_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oos_mistakes(file, mapping):\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    # reorder columns\n",
    "    df = df[[\"Actual\", \"Prediction\", \"File\"]]\n",
    "    # Create separate dataframe of misclassified images\n",
    "    misclassified_images = df[df[\"Prediction\"] != df[\"Actual\"]]\n",
    "    # Change integers to labels\n",
    "    misclassified_images['Prediction']=misclassified_images['Prediction'].map(mapping)\n",
    "    misclassified_images['Actual']=misclassified_images['Actual'].map(mapping)\n",
    "    # Create separate dataframe of correct classifications\n",
    "    correct_classifications = df[df[\"Prediction\"] == df[\"Actual\"]]\n",
    "    # Change integers to labels\n",
    "    correct_classifications['Prediction']=correct_classifications['Prediction'].map(mapping)\n",
    "    correct_classifications['Actual']=correct_classifications['Actual'].map(mapping)\n",
    "    # Summarise mislcassifications and create confusion matrix\n",
    "    df.drop(columns='File', inplace=True)\n",
    "    conf_mat = confusion_matrix(df['Actual'], df['Prediction'])\n",
    "    mapping_list = list(mapping.keys())\n",
    "    predictions_list = df['Prediction'].unique()\n",
    "    # correct size of confusion matrix to account for values not predicted\n",
    "    missing_list = [item for item in mapping_list if item not in predictions_list]\n",
    "    for val in missing_list:\n",
    "        conf_mat = np.insert(conf_mat, val, np.zeros((1,conf_mat.shape[0])), 0)\n",
    "        conf_mat = np.insert(conf_mat, val, np.zeros((1,1)), 1)\n",
    "    df = df.groupby(['Actual', 'Prediction']).size().reset_index()\n",
    "    df.rename(columns={0:'Count'}, inplace=True)\n",
    "    df['Prediction']=df['Prediction'].map(mapping)\n",
    "    df['Actual']=df['Actual'].map(mapping)\n",
    "    return df, misclassified_images, correct_classifications, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_oos_mapping = create_mappings(benchmark_oos_evaluation_df)\n",
    "canidae_oos_mapping = create_mappings(canidae_oos_evaluation_df)\n",
    "felinae_oos_mapping = create_mappings(felinae_oos_evaluation_df)\n",
    "sciuridae_oos_mapping = create_mappings(sciuridae_oos_evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34999e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_oos_mistakes_df, benchmark_oos_mistake_files, benchmark_oos_correct, benchmark_oos_conf_mat = oos_mistakes(\"./Results/output_benchmark_oos_mistakes.csv\",\n",
    "                                                                                benchmark_oos_mapping)\n",
    "canidae_oos_mistakes_df, canidae_oos_mistake_files, canidae_oos_correct, canidae_oos_conf_mat = oos_mistakes(\"./Results/canidae_output_canidae_oos_mistakes.csv\",\n",
    "                                                                          canidae_oos_mapping)\n",
    "felinae_oos_mistakes_df, felinae_oos_mistake_files, felinae_oos_correct, feliane_oos_conf_mat = oos_mistakes(\"./Results/felinae_output_felinae_oos_mistakes.csv\",\n",
    "                                                                      felinae_oos_mapping)\n",
    "sciuridae_oos_mistakes_df, sciuridae_oos_mistake_files, sciuridae_oos_correct, sciuridae_oos_conf_mat = oos_mistakes(\"./Results/sciuridae_output_sciuridae_oos_mistakes.csv\",\n",
    "                                                                            sciuridae_oos_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be22b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(benchmark_oos_mistakes_df, 'results_images/benchmark_oos_mistakes_df.png')\n",
    "conf_matrix(benchmark_oos_conf_mat, list(benchmark_test_evaluation_df.index), \n",
    "            \"results_images/benchmark_oos_conf_mat.png\")\n",
    "dfi.export(canidae_oos_mistakes_df, 'results_images/canidae_oos_mistakes_df.png')\n",
    "conf_matrix(canidae_oos_conf_mat, list(canidae_test_evaluation_df.index), \n",
    "             \"results_images/canidae_oos_conf_mat.png\")\n",
    "dfi.export(felinae_oos_mistakes_df, 'results_images/felinae_oos_mistakes_df.png')\n",
    "conf_matrix(feliane_oos_conf_mat, list(felinae_test_evaluation_df.index),\n",
    "             \"results_images/felinae_oos_conf_mat.png\")\n",
    "dfi.export(sciuridae_oos_mistakes_df, 'results_images/sciuridae_oos_mistakes_df.png')\n",
    "conf_matrix(sciuridae_oos_conf_mat, list(sciuridae_test_evaluation_df.index),\n",
    "             \"results_images/sciuridae_oos_conf_mat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose random images from dataframe\n",
    "\n",
    "def random_files(df_list):\n",
    "    df = df_list[0].sample(1)\n",
    "    for dataframe in df_list[1:]:\n",
    "        new_df = pd.concat([df,dataframe.sample(1)])\n",
    "        df = new_df\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function to create dataframe of misclassified files\n",
    "\n",
    "misclassified_files = random_files([benchmark_mistake_files, benchmark_oos_mistake_files, canidae_mistake_files,\n",
    "             canidae_oos_mistake_files, felinae_mistake_files, felinae_oos_mistake_files,\n",
    "             sciuridae_mistake_files, sciuridae_oos_mistake_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change display settings so entire dataframe is visible\n",
    "\n",
    "misclassified_files.style.set_properties(subset=['File'], **{'width-min': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat with correct classifications\n",
    "\n",
    "correct_files = random_files([benchmark_correct, benchmark_oos_correct, canidae_correct,\n",
    "             canidae_oos_correct, felinae_correct, felinae_oos_correct,\n",
    "             sciuridae_correct, sciuridae_oos_correct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d936f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_files.style.set_properties(subset=['File'], **{'width-min': '300px'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
